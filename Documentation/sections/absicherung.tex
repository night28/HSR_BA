\section{Absicherung}

\subsection{Architektur und Design}
Die SDA Topologie sollte nach den gleichen Designprinzipien und Best Practices aufgebaut werden, welche auch ein Campus-Design verfolgt. Das folgende Beispiel zeigt die physikalische Topologie eines dreistufigen Campus-Designs, bei dem alle Komponenten innerhalb der Fabric redundant sind.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\linewidth]{img/Absicherung/SDA-Architektur}
	\caption{SDA Topologie \cite{sda-designguide-sept2018} }
	\label{fig:SDA Topologie}
\end{figure}

Analog zu dieser wurde die von der Studienarbeit übernommene Architektur weiter angepasst und wenn möglich die Komponenten inklusive Verkabelung redundant ausgelegt (siehe Abbildung \ref{fig:Architektur}: Architektur). \\

Ebenfalls zu beachten sind die Deployment Capabilities, welche als Border und Control Plane Node die Catalyst 3850 empfehlen. Darum werden in der Architektur die Catalyst 9300 als Intermediate oder Edge Nodes eingesetzt.

\subsection{ENCS 5400}

Um mit möglichst kleinem Hardwareeinsatz eine maximale Autonomie der Aussenstandorte zu ermöglichen, wird pro Aussenstelle ein oder mehrere ENCS 5400 eingesetzt. 
Dabei handelt es sich um eine Virtualisierungsplattform basierend auf KVM. Des Weiteren beinhaltet die Appliance einen Switch. 
Auf diesem System können virtuelle Router, Firewalls, WLCs und mittels Third Party Images viele weitere Dienste betrieben werden. Für spezifische Anwendungen können eigene Images erstellt werden, wodurch die Plattform enorm vielseitig einsetzbar ist.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\linewidth]{img/Absicherung/ENCS_Architecture.jpg}
	\caption{ENCS Architecturee \cite{enterprise-network-functions-virtualization-faq} }
	\label{fig:ENCS Architecture}
\end{figure}

\subsubsection{Image erstellen}

In einem ersten Schritt muss ein Disk Image für den gewünschten Service vorliegen. In unserem Fall wurde diese via Virtualbox erstellt, sodass alle nötigen Pakete bereits installiert werden konnten. Da das Image im Format qcow2 vorliegen muss, wird das vdi Image entsprechend konvertiert.

\begin{lstlisting}[language=bash]
qemu-img convert -f vdi -O qcow2 Ubuntu18.04_Branch.vdi \
Ubuntu18.04_Branch.qcow2
\end{lstlisting}

\paragraph{Image Packaging}

Damit das Image verwendet werden kann, muss ein Package erstellt werden. Dabei handelt es sich um ein Archiv, bestehend aus dem Disk Image, sowie einem XML File, in welchem Metadaten, wie die Hardwareanforderungen und die verschiedenen Profile definiert sind.
Das Package kann manuell oder mittels NFVIS Web Interface erstellt werden.

Im Web-Interface wird dies unter \textit{VM Life Cycle $\rightarrow$ Image Repository $\rightarrow$ Image Packaging} erstellt.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\linewidth]{img/Absicherung/ENCS-Image-Packaging.png}
	\caption{ENCS Image Packaging}
	\label{fig:ENCS Image Packaging}
\end{figure}

\paragraph{Image Konfiguration}

Anschliessend werden die Metadaten für das Image definiert. Diese sind schlussendlich im XML File zu finden.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\linewidth]{img/Absicherung/ENCS-Image-Creation.png}
	\caption{ENCS Image Konfiguration}
	\label{fig:ENCS Image Konfiguration}
\end{figure}

Des Weiteren werden die verschiedenen Profile definiert. Diese können später beim Deployment ausgewählt weden, sodass für verschiedene Grössen eines Branches ein passendes Profil ausgewählt werden kann.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\linewidth]{img/Absicherung/ENCS-Image-Profile.png}
	\caption{ENCS Image Profile}
	\label{fig:ENCS Image Profile}
\end{figure}

\paragraph{Image Deployment}

Das Deployment des Images soll via DNA Center funktionieren. Aus diesem Grund muss das zuvor erstellte Archiv heruntergeladen und anschliessend ins DNA Center importiert werden. \textit{Design $\rightarrow$ Image Repository $\rightarrow$ Virtual $\rightarrow$ Import}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\linewidth]{img/Absicherung/DNA-Center-Image-Import.png}
	\caption{DNA Center Image Import}
	\label{fig:DNA Center Image Import}
\end{figure}



\subsection{DNA Center}
Das DNA Center unterstützt eine Clusterkonfiguration mit einem oder drei Hosts. Im Optimalfall sollte das DNA Center in einem Cluster mit drei Nodes installiert werden. Der Cluster bietet sowohl Software als auch Hardware mit hoher Verfügbarkeit. Das DNA Center bietet einen Mechanismus zum Verteilen der Verarbeitung und Datenbankreplikation auf mehrere Hosts. Durch das Clustering werden Ressourcen und Funktionen gemeinsam genutzt und es werden hohe Verfügbarkeit und Skalierbarkeit ermöglicht. \\

Die folgende Abbildung zeigt die empfohlenen Verbindungen für einen DNA Center Cluster mit drei Knoten. 
\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\linewidth]{img/Absicherung/DNAC-Cluster}
	\caption{DNA Center Cluster \cite{DNAC-Cluster} }
	\label{fig:DNA Center Cluster}
\end{figure}
Alle bis auf eine der Verbindungen für jeden Knoten im drei Nodes Cluster sind die gleichen wie für den einzelnen Node und verwenden dieselben Ports. Die Ausnahme ist der Cluster-Port (Port 2, enp10so, Network Adapter 1), der erforderlich ist, damit jeder Host im Cluster mit drei Knoten mit den anderen kommunizieren kann. \\

Für Clusterbereitstellungen mit mehreren Knoten müssen sich alle Cluster Nodes im selben Netzwerk und am selben Standort befinden. Die Appliance unterstützt keine Verteilung von Knoten über mehrere Netzwerke oder Standorte. Diese Limitierung der Bereitstellung, sowie auch die maximale Anzahl der unterstützen Nodes wird sich hoffentlich in Zukunft noch ändern.

\subsection{LISP Map Server}
Es gibt zwei Betriebsarten für einen LISP Map-Server(MS): 
\begin{itemize}
	\item als Map-Resolver(MR), der Map-Requests von einem ITR entgegennimmt und das EID-zu-RLOC-Mapping mit Hilfe der verteilten Mapping-Datenbank auflöst
	\item als Map-Server(MS), der autoritative EID-zu-RLOC-Mappings von einem ETR lernt und in der Datenbank veröffentlicht
\end{itemize}

Zur Bereitstellung gibt es zwei Varianten. Zum einem kann es einen redundanten globalen MSMR geben, oder es werden pro Fabric ein MSMR implementiert.

\subsubsection{Redundante MS / MR Bereitstellung}

Es wird empfohlen, redundante Standalone MS- und MR-Systeme mit den MS / MR-Funktionen auf demselben Gerät bereitzustellen. Wenn redundante eigenständige MS / MR implementiert werden, müssen sich alle xTRs bei beiden MS registrieren, so dass jeder eine konsistente Sicht auf den registrierten LISP EID-Namespace hat. Für Map-Resolver-Funktionalität ist die Verwendung einer Anycast-IP-Adresse wünschenswert, da dadurch die Mapping-Lookup-Leistung verbessert wird, indem der MR ausgewählt wird, der dem anfordernden ITR am nächsten ist.

\begin{figure}[H]
	\centering
	\includegraphics[width=1\linewidth]{img/Absicherung/LISP-Example}
	\caption{Redundante MS / MR Bereitstellung \cite{LISP-mobility} }
	\label{fig:Redundante MS / MR Bereitstellung}
\end{figure}

\subsubsection{Co-Lokalisierung von MS / MR und xTR Funktionalitäten}

Ein weiteres Beispiel ist die Co-Lokalisierung  von MS / MR- und xTR-Funktionalitäten. Das co-lokalisierte Modell ist besonders vorteilhaft, da es die Gesamtzahl verwalteter Geräte reduziert, die zum Ausrollen einer LISP Host Mobility-Lösung erforderlich sind. 

Die erforderliche Konfiguration würde aber in beiden Szenarien identisch bleiben, indem eindeutige IP-Adressen genutzt werden, um die Map-Server und eine Anycast-IP-Adresse für den Map-Resolver zu identifizieren.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\linewidth]{img/Absicherung/LISP-Example2}
	\caption{Co-Lokalisierung von MS / MR und xTR Funktionalitaeten \cite{LISP-mobility} }
	\label{fig:Co-Lokalisierung von MS / MR und xTR Funktionalitaeten}
\end{figure}

Es kann also ein redundantes eigenständiges MS / MR-Modell bereitgestellt werden, indem dedizierte Systeme zur Durchführung dieser Mapping-Funktionen genutzt werden (siehe Abbildung 2.9) oder alternativ können die MS- und MR-Funktionen gleichzeitig auf dem Netzwerkgerät, welches bereits die xTR-Rolle ausführt, implementiert werden (siehe Abbildung 2.10).

\subsubsection{Anwendung}
Damit bei einem Ausfall eines MSMR nicht alle Fabrics betroffen sind, macht es Sinn pro Fabric einen redundanten MSMR zu implementieren. So können auf einem xTR eine oder mehrere MSMR-Adressen konfiguriert werden.

Abfragen, die ein EID-zu-RLOC-Mapping durchführen, sind datengesteuert. Dieses Verhalten bedeutet, dass ein neuer Datentransfer zwischen LISP-Sites ein Mapping-Lookup erfordert, was dazu führt, dass der Datenversand wird gestoppt, bis ein Mapping durchgeführt wurde. Dies Verhalten ist analog zum DNS-Protokoll und ermöglicht LISP die folgenden Funktionen in einer dezentralen Datenbank mit EID-zu-RLOC-Mappings zu betreiben. 

Die Replikation der gesamten (potenziell umfangreichen) Datenbank ist unnötig, da auf Mappings bei Bedarf zugegriffen wird, genau wie im DNS muss ein Host nicht die komplette Domänendatenbank kennen. Tunnelrouter verwalten den Map-Cache der zuletzt verwendeten Mappings, um die Leistung des Systems zu verbessern.

\paragraph{LISP Client Registration}
Wird ein noch unbekannter neuer Client an die Fabric angeschlossen, sendet der ITR einen Map-Request an einen bekannten MR, wenn es ein EID-zu-RLOC-Mapping benötigt, das noch nicht in seinem lokalen Map-Cache vorhanden ist.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\linewidth]{img/Absicherung/LISP-ClientRegistration}
	\caption{LISP Client Registration}
	\label{fig:LISP Client Registration}
\end{figure}

\paragraph{LISP Host Resolution}
Will ein Client mit einem noch unbekannten anderen Client kommunizieren, so sucht sein ITR zuerst in seinem lokalen Map-Cache nach einem Eintrag. Ist noch kein Eintrag zum Client2 vorhanden, so schickt der ITR ein Map-Request zu seinem MR. Der MS sendet dann den originalen Map-Reguest an den zuletzt registrierten ETR. Da Client2 noch am ETR angeschlossen ist, sendet dieser einen Map-Reply an den ITR, welcher die angefragten Mapping-Informationen enthält.

Bei einem Ping werden die initialen Pakete verworfen, bis die Host Resolution abgeschlossen ist. 

\begin{figure}[H]
	\centering
	\includegraphics[width=1\linewidth]{img/Absicherung/LISP-HostResolution-Fail}
	\caption{LISP Host Resolution}
	\label{fig:LISP Host Resolution}
\end{figure}

\paragraph{Host Mobility}
Die Host Mobility ist ähnlich wie die Client Registration, da der Client sich beim neuen FE2 zuerst registriert und die neuen Informationen schliesslich vom CP1 dann an den alten FE1 zur Aktualisierung weitergegeben werden.

\begin{figure}[H]
	\centering
	\includegraphics[width=1\linewidth]{img/Absicherung/LISP-HostMobility-Fail}
	\caption{LISP Host Mobility}
	\label{fig:LISP Host Mobility}
\end{figure}

Wenn ein dynamischer EID zwischen Rechenzentrumsstandorten wechselt, müssen die lokalen LISP Host Mobility-xTRs ihre Existenz erkennen. 

Das für LISP Host Mobility konfigurierte xTR erkennt ein Host Mobility Ereignis wenn:
\begin{enumerate}
	\item Er empfängt ein IP-Datenpaket von einer Quelle (des neu eingetroffenen Workloads), die aus Routing-Sicht nicht über die Schnittstelle erreichbar ist, auf der das Paket empfangen wurde.
	\item Die Quelle entspricht der auf die Schnittstelle angewendeten Dynamic-EID-Konfiguration.
\end{enumerate}


\subsubsection{Ausfall MSMR}
Bei einem Ausfall des MSMR kann keine Host Registration mehr vorgenommen werden. Das heisst neue Clients können keine Verbindung zum restlichen Netzwerk aufbauen. Bestehende Clients können nur mit Clients welche am selben FE angeschlossen sind kommunizieren. Dies jedoch nur solange ihr Eintrag im lokalen Map-Cache bestehen bleibt. Der TTL der Einträge im Map-Cache beträgt per default einen Tag. 

Da jeder xTR seinen eigenen Map-Cache hat und kann sich sein Inhalt innerhalb derselbe LISP-Site unterscheiden. Daher können xTr leicht schwere Paketausfälle erleiden und mit LISP Control Messages geflutet werden.  

\paragraph{MS}
Es sollten mehrere Map-Server vorhanden und eingetragen sein. Optimalerweise ist mindestens ein Map-Server pro Site vorhanden. Sollte der erste Map-Server nicht erreichbar sein, so wird der zweite Map-Server verwendet.

\paragraph{MR}
Für den Map-Resolver sollte eine Anycast IP-Adresse verwendet werden. So werden die Pakete gesendet und über einen verfügbaren Map-Resolver zum Ziel weitergeleitet. Auch hier ist es sinnvoll, mindestens einen Map-Resolver pro Site bereitzustellen.

\subsection{ISE / Radius}

\subsubsection{ISE Cluster}

Um die Verfügbarkeit des ISE zu erhöhen, kann diese in einem Cluster, bestehend aus einem Master und mehreren Slaves betrieben werden. Dies erhöht die Ausfallsicherheit aller Services auf dem ISE. Damit diese gewährleistet sind, wenn ein Aussenstandort die Verbindung zum Hauptsitz verliert, müsste aber an jedem Standort ein ISE Slave existieren. Bei sehr vielen Standorten ist dies auf Grund des Verwaltungsaufwands und der Kosten für viele ISE Instanzen nicht praktikabel. Ein Cluster am Hauptstandort ist aber sicherlich sinnvoll.

\subsubsection{Read Only Radius Server an Aussenstandorten}

In Aussenstellen wird ein Read Only Radius Server betrieben, damit die Network Access Devices auch im Falle eines Unterbruchs der Verbindung zum Hauptsitz einen Radius Server zur Verfügung haben. Hier könnte beispielsweise Freeradius eingesetzt werden. Da Freeradius seine Informationen nicht direkt vom ISE beziehen kann, muss vom ISE ein externer Radius Server verwendet werden, der eine Replikation unterstützt. Dies kann beispielsweise Freeradius sein.

\subsubsection{Deployment Size and Scaling Recommendations}

Die nachfolgenden Tabellen zeigen die Performance und Scalability Metriken für Radius Sessions, Passive Identity, Easy Connect, pxGrid und ISE Services.

\begin{figure}[H]
	\centering
	\includegraphics[width=1\linewidth]{img/Absicherung/ISE-MaxRadiusScaling}
	\caption{ISE - Maximum RADIUS Scaling by Deployment with Maximum Passive Identity/Easy Connect Scaling by Deployment Size \cite{ise-scale}}
	\label{fig:ISE - Maximum RADIUS Scaling}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=1\linewidth]{img/Absicherung/ISE-pxGridServicesScalability}
	\caption{ISE - Scalability with pxGrid Services \cite{ise-scale}}
	\label{fig:ISE - Scalability with pxGrid Service}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=1\linewidth]{img/Absicherung/ISE-PlatformpxGridV2Scaling}
	\caption{ISE - pxGrid v2 Scaling per Dedicated pxGrid Node \cite{ise-scale}}
	\label{fig:ISE - pxGrid v2 Scaling per Dedicated pxGrid Node}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\linewidth]{img/Absicherung/ISE-pxGridV2}
	\caption{ISE - Platform eXchange Grid (pxGrid v2) Scaling \cite{ise-scale}}
	\label{fig:ISE - Platform eXchange Grid (pxGrid v2) Scaling}
\end{figure}


\paragraph{ISE}
An einem Hauptstandort wird der primäre ISE Node und ein Dedicates ISE Syslog oder Logging Node betrieben. In einem grossen zentralisierten Netzwerk sollte ein Load Balancer verwendet werden, der die Bereitstellung von AAA-Clients vereinfacht. Die Verwendung eines Load Balancers erfordert nur einen einzigen Eintrag für die AAA-Server und der Load Balancer optimiert das Routing von AAA-Anfragen an die verfügbaren Server. Um einen Single Point of Failure zu vermeiden, sollten zwei Load Balancer eingesetzt werden. Für diese Konfiguration müssen jedoch auf den erforderlichen Geräten zwei AAA-Server-Einträge engerichtet werden.

Weitere verteilte grosse Standorte können über eine eigene AAA-Infrastruktur für eine optimale AAA-Performance verfügen. Ein zentralisiertes Verwaltungsmodell hilft bei der Aufrechterhaltung einer konsistenten, synchronisierten AAA-Richtlinie. Ein zentralisiertes Konfigurationsmodell verwendet einen primären Cisco ISE Node mit einem sekundären Cisco ISE Node. Cisco empfiehlt für jeden Standort eine separate Monitoring Persona auf dem Cisco ISE Node, aber jeder entfernte Standort sollte seine eigenen einzigartigen Netzwerkanforderungen beibehalten.


\paragraph{Freeradius}

Freeradius wird in einem Master / Slave Setup betrieben. Am Hauptstandort befindet sich der Radius Master und an allen Aussenstandorten ist ein Slave verfügbar. Die Replikation wird mittels MySQL Replikation sichergestellt.

\paragraph{Network Access Devices}

Auf den Network Devices muss der Radius Server vom jeweiligen Standort konfiguriert sein. Dies kann im DNA Center unter \textit{Design $\rightarrow$ Network Settings $\rightarrow$ AAA Server} konfiguriert werden.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\linewidth]{img/Absicherung/DNA_Center_AAA-Server.png}
	\caption{DNA Center - AAA Server }
	\label{fig:DNA Center - AAA Server}
\end{figure}


\subsection{SGT Access List}
Beim Erstellen von SGTs über die DNAC-Benutzeroberfläche wird die ISE-Benutzeroberfläche cross-gestartet und die Aufgabe wird dort abgeschlossen. ISE verwaltet alle skalierbaren Gruppeninformationen, die später in DNAC für die Richtlinienerstellung verwendet werden. Obwohl die Richtlinien und die entsprechenden Verträge bei DNAC erstellt werden, werden beide über die REST-API der ISE an die ISE zurückgemeldet. ISE dient dann als zentrale Anlaufstelle für SGTs, Richtlinien und Verträge (SGACLs), die dann dynamisch an die Netzwerkinfrastruktur verteilt werden.
Die Segmentierung innerhalb der SDA wird durch die kombinierte Verwendung von virtuellen Netzwerken (VN), die mit VRFs gleichgesetzt sind, als auch TrustSec Scalable Group Tags (SGTs) ermöglicht. Während die Segmentierung durch die Verwendung von absichtsgesteuerten oder speziell dafür entwickelten virtuellen Netzwerken allein erreicht werden kann, bieten die Cisco Trustsec SGTs eine logische Segmentierung basierend auf der Gruppenmitgliedschaft. Cisco bietet eine zusätzliche Granularitätsebene, mit der mehrere SGTs innerhalb eines einzigen VN verwenden können, die eine Mikrosegmentierung innerhalb des VN ermöglicht. Die Segmentierung erfolgt innerhalb des SDA sowohl auf Makro- als auch auf Mikroebene durch virtuelle Netzwerke bzw. SGTs. Die Richtlinien und die damit verbundenen Verträge werden im DNAC konfiguriert und dann über die REST-API an die ISE übermittelt. ISE aktualisiert dann die Edge Nodes mit nur den Richtlinien für SGTs, die den angeschlossenen Geräten zugeordnet sind. Die Durchsetzung erfolgt beim Austritt, wenn das Ziel verbunden ist. \cite{sda-segmentation-may2018}

Kurze Zusammenfassung der damit benötigten Komponenten:
\begin{itemize}
	\item Sicherheitsgruppe (SG) - Eine Gruppe von Benutzern, Endpunktgeräten und Ressourcen, die Zugriffssteuerungsrichtlinien gemeinsam nutzen. SGs werden vom Administrator in Cisco ISE definiert. Wenn neue Benutzer und Geräte zur TrustSec Domäne hinzugefügt werden, ordnet Cisco ISE diese neuen Entitäten den entsprechenden Sicherheitsgruppen zu.
	\item Security Group Tag (SGT) - Der TrustSec-Dienst weist jeder Sicherheitsgruppe eine eindeutige 16-Bit-Sicherheitsgruppennummer zu, deren Gültigkeitsbereich innerhalb einer TrustSec Domäne global ist. Die Anzahl der Sicherheitsgruppen im Switch ist auf die Anzahl der authentifizierten Netzwerkentitäten beschränkt. Die Sicherheitsgruppennummern müssen nicht manuell konfigurieren. Sie werden automatisch generiert, aber es gibt auch die Möglichkeit eine Reihe von SGTs für die IP-zu-SGT-Zuordnung zu reservieren.
	\item Security Group Access Control List (SGACL) - Mit SGACLs kann der Zugriff und die Berechtigung basierend auf den zugewiesenen SGTs gesteuert werden. Die Gruppierung von Berechtigungen in einer Rolle vereinfacht die Verwaltung von Sicherheitsrichtlinien. Beim Hinzufügen von Geräten wird einfach eine oder mehrere Sicherheitsgruppen zugewiesen und diese erhalten sofort die entsprechenden Berechtigungen. Die Sicherheitsgruppen können geändert werden, um neue Berechtigungen einzuführen oder die aktuellen Berechtigungen einzuschränken.
	\item Security Exchange Protocol (SXP) - Das SGT Exchange Protocol (SXP) ist ein Protokoll, das für den TrustSec Dienst entwickelt wurde, um die IP-SGT-Bindungen auf Netzwerkgeräte zu übertragen, die keine SGT-fähige Hardwareunterstützung für Hardware bieten, die SGT / SGACL unterstützt.
\end{itemize}


\begin{figure}[H]
	\centering
	\includegraphics[width=1\linewidth]{img/Absicherung/SGT-FusionFirewall}
	\caption{Policy Enforcement mit einer Fusion Firewall}
	\label{fig:Policy Enforcement mit einer Fusion Firewall}
\end{figure}

Oben dargestellte Abbildung veranschaulicht die Verwendung einer Fusion Firewall, welche für die Kommunikation zwischen virtuellen Netzwerken sowie für den Verkehr an einem anderen Ort im Netzwerk. Mithilfe von Standard ACLs oder gruppenbasierten Richtlinien mit SGTs werden Firewall-Regeln in der Fusions-Firewall definiert, die den Datenverkehr zwischen Endpunkten steuert.

\subsubsection{Failover}
Die Absicherung der Access Listen kann beispielsweise nach folgenden Beispielen erarbeitet werden, müssen jedoch noch genauer getestet werden:

\begin{itemize}
	\item Beispiel1: lokal auf 9300 SGACLs alle fünf Minuten sichern und diese bei Ausfall wieder eintragen. Nachteil: Host-Mobility funktioniert nicht, aber lokal kann weitergearbeitet werden.
	\item Beispiel2: global komplett alle SGACLs auf alle Geräte verteilen. Vorteil: Alle Geräten haben jederzeit alle Einträge. Nachteil: Die Maximum Scale Recommendations der SGACLs sind pro Gerät anders. C3850 - 1500 SGACLs, C9300 - 5000 SGACLs
\end{itemize}

Es können TrustSec SXP Speaker/Listener auf den Catalyst 3850 und C9300 definiert werden, indem die Catalyst 3850 als Gateway für die Catalyst 9300 Listener angegeben werden.

\subsection{Border Node}

Der gesamte Verkehr der die Fabric betritt oder verlässt, durchläuft diesen Knoten. Um einen Single Point of Failure zu vermeiden, sollten immer mindestens zwei Border Nodes pro Site zur Verfügung stehen. Nach den aktuellen Maximum Scale Recommendations können maximal 4 Border Nodes Site implementiert werden.

Border Nodes implementieren die folgenden Funktionen, welche bei einem Ausfall in Mitleidenschaft gezogen werden könnten\cite{sda-designguide-sept2018}:
\begin{itemize}
	\item Ankündigung von EID-Subnetzen
	\item Fabric-Domänenausstiegspunkt
	\item Mapping der LISP-Instanz auf VRF
	\item Richtlinienzuordnung
\end{itemize}

\subsubsection{Ankündigung von EID-Subnetzten}
SD-Access konfiguriert Border Gateway Protocol (BGP) als bevorzugtes Routingprotokoll, das für die Ankündigung der EID-Präfixe außerhalb der Fabric verwendet wird, und der für EID-Subnetze von außerhalb der Fabric bestimmte Verkehr wird durch die Grenzknoten geleitet. Diese EID-Präfixe werden nur in den Routingtabellen am Rand angezeigt. Im gesamten Rest der Fabric wird auf die EID-Informationen über die Fabric-Steuerebene zugegriffen.

\subsubsection{Fabric-Domänenausstiegspunkt}
Der externe Fabric Border ist das Gateway des letzten Auswegs für die Fabric Edge Nodes. Dies wird mithilfe der LISP Proxy Tunnel Router-Funktionalität implementiert. Möglich sind auch interne Fabric Borders, die mit Netzwerken mit einem genau definierten Satz von IP-Subnetzen verbunden sind, wodurch die Ankündigung dieser Subnetze in der Fabric hinzugefügt werden muss.

\subsubsection{Mapping der LISP-Instanz auf VRF}
Der Fabric Border Node kann die Netzwerkvirtualisierung mithilfe von externen VRF-Instanzen von innerhalb der Fabric auf die Fabric-Außenseite ausweiten, um die Virtualisierung beizubehalten.

\subsubsection{Richtlinienzuordnung}
Der Fabric Border Node bildet auch SGT-Informationen aus der Fabric ab, die beim Verlassen dieser Fabric entsprechend verwaltet werden. SGT-Informationen werden vom Fabric Border Node an das ausserhalb der Fabric liegende Netzwerk weitergegeben, indem entweder die Tags mithilfe von SGT Exchange Protocol (SXP) zu Cisco-fähigen Geräten transportiert werden, oder indem SGTs direkt in einem Cisco-Metadatenfeld in einem Paket zugeordnet werden Inline-Tagging-Funktionen für Verbindungen zum Grenzknoten implementiert.

\subsection{Fusion Router}
Die Fusion Router stellen die Verbindungen zwischen den einzelnen Fabrics, dem Internet, sowie die Verbindung zum Legacy Netzwerk in dem sich beispielsweise das DNAC und der ISE befinden.Es werden mehrere Fusion Router verwendet, um die nötige Ausfallsicherheit zu gewährleisten. Die Fusion Router und Border Nodes sind im Optimalfall in einem Full-Mesh verkabelt. Für das Routing zwischen den Fusion Routern, sowie den Border Nodes kommt BGP zum Einsatz.

Die Topologie wurde analog anhand der nachfolgenden Validation Topologie von Cisco aufgebaut.
\begin{figure}[H]
	\centering
	\includegraphics[width=1\linewidth]{img/Absicherung/FusionRouter-ValidationTopology}
	\caption{Fusion Router Validation Topology \cite{sda-deploymentguide-oct2018} }
	\label{fig:Fusion Router Validation Topology}
\end{figure}


\subsection{DHCP}

\subsubsection{Infoblox}
Um die Krisenresistenz am Hauptstandort zu erhöhen, wird Infoblox als Multi-Node Cluster betrieben. Dadurch kann ein einzelner Node ausfallen, ohne dass dies einen Impact auf den Betrieb am Hauptstandort hat. 

\subsubsection{Aussenstellen}
Da sich Infoblox nicht in Kombination mit 3rd Party Software in einem Cluster oder einer Failoverlösung betreiben lässt, müssen an Aussenstandorten eigenständige DHCP Server betrieben werden, um deren Autonomie sicherzustellen. Da Infoblox den isc-dhcp-server für seine DHCP Services verwendet, wird dieser auch an den Aussenstellen eingesetzt. 
Für wichtige Aussenstandorte kann der isc-dhcp-server in einem Master-Slave Cluster betrieben werden. Somit isch an diesen Standorten ebenfalls eine Ausfallsicherheit gewährleistet.

\subsubsection{DNA Center}
Im DNA Center muss für jeden Aussenstandort der jeweilige DHCP Server konfiguriert werden. Im Falle eines DHCP Clusters können hier auch mehrere Einträge erstellt werden.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\linewidth]{img/Absicherung/DNA_Center_DHCP-Server.png}
	\caption{DNA Center - DHCP Server }
	\label{fig:DNA Center - DHCP Server}
\end{figure}

\subsection{DNS}

\subsubsection{Infoblox HA Cluster}


\subsubsection{Read Only DNS Server an Aussenstandorten}
	
Damit Aussenstellen nicht auf DNS Server des Hauptstandortes angewiesen sind, kann in jedem Standort ein Read-Only Server betrieben werden. Somit funktioniert die Namensauflösung auch im Falle eines Kommunikationsverlusts zum Hauptstandort. 

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\linewidth]{img/Absicherung/DNS_Sequenzdiagram.png}
	\caption{DNS Sequenzdiagramm}
	\label{fig:DNS Sequenzdiagramm}
\end{figure}
\paragraph{Infoblox}

Damit die Read-Only Server stets über die aktuellsten DNS Zonen verfügen, müssen die Informationen von Infoblox auf diese repliziert werden. In diesem Fall wird dafür der Zone Transfer verwendet. Dazu muss dies in Infoblox für alle Slave Server erlaubt werden. 

Dies wird in Infoblox via \textit{Grid $\rightarrow$ DNS $\rightarrow$ Infoblox Instanz $\rightarrow$ Edit $\rightarrow$ Zone Transfers} ausgeführt.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\linewidth]{img/Absicherung/Infoblox_Zone_Transfer.png}
	\caption{Infoblox Zone Transfer}
	\label{fig:Infoblox Zone Transfer}
\end{figure}

\paragraph{DNS Slaves}

Auf den Slaves an den jeweiligen Aussenstandorten müssen die Zonen als Slave Zonen konfiguriert sein und Infoblox muss als Master konfiguriert werden. Dadurch können die Zonen vom Master auf den Slave transferiert werden. Der Slave aktualisiert alle Slave Zonen in regelmässigen Abständen. Dieser Intervall wird in der Zone im SOA Record mit dem "Refresh" Parameter definiert.
Zusätzlich kann auf dem Master konfiguriert werden, dass alle Slaves mittels "Notify" informiert werden, sobald sich eine Zone ändert, worauf der Slave die aktuellsten Informationen für diese Zone abruft. Somit ist sichergestellt, dass alle Server an Aussenstandorten stets über eine aktuelle Konfiguration verfügen.

\paragraph{Clients}

Auf den Cliens ist es wichtig, dass alle nötigen DNS Server in der korrekten Reihenfolge konfiguriert werden. Als erster Server soll der DNS Server am jeweiligen Standort eingetragen werden, um Antwortzeiten möglichst gering zu halten. Sollte dieser ausfallen, kann auf Infoblox oder weitere DNS Server zurückgegriffen werden.


\subsection{Scheduled Software Updates}
Alle Netzwerk Devices die über das DNA Center verwaltet werden, erhalten ihre aktuellsten Software Images auch von diesem. Die Images haben teils sehr unterschiedliche Grössen (von 200 MB bis zu 1 GB), weshalb es wichtig ist, dass sich diese schon vor einem Upgrade auf den Geräten befinden. 


\subsubsection{Provision Software Images}
Bevor ein Software Image auf ein Gerät übertragen wird, überprüft das DNA Center auf den einzelnen Geräten, ob dieses auch bereit für ein Update ist. Dazu gehören zum Beispiel das Überprüfen des Geräteverwaltungsstatus, Überprüfung der SCP- und HTTPS-Dateiübertragung, Festplattenspeicher usw. Wenn eine Vorprüfung fehlschlägt, kann die Aktualisierung des Software Images nicht durchgeführt werden. \\

Nun kann das Verteilen der Software Images nach folgendem Ablauf gestartet werden:
\begin{enumerate}
	\item Das Verteilen der Software Images kann im DNA Center unter \textit{Provision} konfiguriert werden.
	\item Nun kann das Gerät, welches aktualisiert werden soll, ausgewählt werden. (Wenn der pre-check erfolgreich war, weisst der Link "Outdatet" in der Spalte "OS Image" ein grünes Häckchen auf.)
	\item In der Dropdown-Liste unter \textit{Actions  $\rightarrow$ Update OS Image} wählen
	\begin{enumerate}
		\item Distribute: Mit einem Klick auf \textit{Now} wird das Verteilen des Images sofort gestartet, oder es wird mit einem Klick auf \textit{Later} an einem spezifischen Zeitpunkt ausgeführt. (Wenn sich das Image bereits auf dem ausgewählten Gerät befindet, wird der Distribute-Vorgang übersprungen und das Image kann direkt aktiviert werden.)
		\item Activate: Mit einem Klick auf \textit{Now} wird die Aktivierung sofort gestartet, oder es wird mit einem Klick auf \textit{Later} an einem spezifischen Zeitpunkt ausgeführt. (Dieser Schritt kann übersprungen werden, wenn zum jetzigen Zeitpunkt nur das Image auf das Gerät verteilt werden sollte.)
		\item Confirm: Mit einem Klick auf \textit{Confirm} wird das Update bestätigt.
	\end{enumerate}
	\item Nun kann unter \textit{Upgrade Status} der aktuelle Vorgang des Image Upgrades beobachtet werden.
\end{enumerate}

Sollte das Verteilen des Software Images nicht funktionieren, in dem es Beispielsweise die Übertragung immer wieder abbricht, so kann das Image auch manuell auf das Gerät kopiert werden. So würde dies bei dem vorher beschriebenen Vorgang \textit{Distribute} automatisch erkennt werden, dass sich das Image schon auf dem Gerät befindet.

